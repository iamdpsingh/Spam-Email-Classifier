{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1745e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "📊 Using matplotlib and seaborn for data visualization\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"📊 Using matplotlib and seaborn for data visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7beceec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Try different separator and quote handling\n",
    "try:\n",
    "    df = pd.read_csv('../data/spamhamdata.csv', \n",
    "                     encoding='latin-1', \n",
    "                     sep=',',\n",
    "                     quotechar='\"',\n",
    "                     on_bad_lines='skip')\n",
    "    print(\"✅ Dataset loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbdb234",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Clean the data\u001b[39;00m\n\u001b[32m     11\u001b[39m df = df.dropna()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.str.lower()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCleaned dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLabel distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preprocessing\n",
    "# Handle different column names\n",
    "if 'v1' in df.columns and 'v2' in df.columns:\n",
    "    df = df[['v1', 'v2']].copy()\n",
    "    df.columns = ['label', 'text']\n",
    "elif 'Category' in df.columns and 'Message' in df.columns:\n",
    "    df = df[['Category', 'Message']].copy()\n",
    "    df.columns = ['label', 'text']\n",
    "\n",
    "# Clean the data\n",
    "df = df.dropna()\n",
    "df['label'] = df['label'].str.lower()\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a319b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Label distribution\n",
    "df['label'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'lightcoral'])\n",
    "axes[0,0].set_title('Distribution of Email Categories', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Category')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Message length distribution\n",
    "df['text_length'] = df['text'].str.len()\n",
    "sns.boxplot(data=df, x='label', y='text_length', ax=axes[0,1])\n",
    "axes[0,1].set_title('Text Length Distribution by Category', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Category')\n",
    "axes[0,1].set_ylabel('Text Length (characters)')\n",
    "\n",
    "# 3. Word count distribution\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "sns.histplot(data=df, x='word_count', hue='label', bins=50, ax=axes[1,0])\n",
    "axes[1,0].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Number of Words')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Pie chart of label distribution\n",
    "df['label'].value_counts().plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])\n",
    "axes[1,1].set_title('Email Category Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n📊 SUMMARY STATISTICS:\")\n",
    "print(f\"Total emails: {len(df)}\")\n",
    "print(f\"Spam emails: {len(df[df['label'] == 'spam'])} ({len(df[df['label'] == 'spam'])/len(df)*100:.1f}%)\")\n",
    "print(f\"Ham emails: {len(df[df['label'] == 'ham'])} ({len(df[df['label'] == 'ham'])/len(df)*100:.1f}%)\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c622c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text analysis with word frequency\n",
    "def analyze_word_patterns(df):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    # Extract words for spam and ham\n",
    "    spam_text = ' '.join(df[df['label'] == 'spam']['text']).lower()\n",
    "    ham_text = ' '.join(df[df['label'] == 'ham']['text']).lower()\n",
    "    \n",
    "    # Get word frequencies\n",
    "    spam_words = re.findall(r'\\b[a-zA-Z]{3,}\\b', spam_text)\n",
    "    ham_words = re.findall(r'\\b[a-zA-Z]{3,}\\b', ham_text)\n",
    "    \n",
    "    spam_counter = Counter(spam_words).most_common(20)\n",
    "    ham_counter = Counter(ham_words).most_common(20)\n",
    "    \n",
    "    # 1. Top spam words\n",
    "    if spam_counter:\n",
    "        words, counts = zip(*spam_counter)\n",
    "        y_pos = np.arange(len(words))\n",
    "        axes[0,0].barh(y_pos, counts, color='lightcoral', alpha=0.8)\n",
    "        axes[0,0].set_yticks(y_pos)\n",
    "        axes[0,0].set_yticklabels(words)\n",
    "        axes[0,0].set_title('Top 20 Words in SPAM Emails', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Frequency')\n",
    "    \n",
    "    # 2. Top ham words\n",
    "    if ham_counter:\n",
    "        words, counts = zip(*ham_counter)\n",
    "        y_pos = np.arange(len(words))\n",
    "        axes[0,1].barh(y_pos, counts, color='lightblue', alpha=0.8)\n",
    "        axes[0,1].set_yticks(y_pos)\n",
    "        axes[0,1].set_yticklabels(words)\n",
    "        axes[0,1].set_title('Top 20 Words in HAM Emails', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Frequency')\n",
    "    \n",
    "    # 3. Character-level analysis\n",
    "    df['exclamation_count'] = df['text'].str.count('!')\n",
    "    df['question_count'] = df['text'].str.count('\\?')\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    \n",
    "    # Exclamation marks\n",
    "    sns.boxplot(data=df, x='label', y='exclamation_count', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Exclamation Marks Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Number of Exclamation Marks')\n",
    "    \n",
    "    # Capital letters ratio\n",
    "    sns.boxplot(data=df, x='label', y='caps_ratio', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Capital Letters Ratio', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Ratio of Capital Letters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"📊 TEXT ANALYSIS INSIGHTS:\")\n",
    "    print(f\"Avg exclamation marks in spam: {df[df['label']=='spam']['exclamation_count'].mean():.2f}\")\n",
    "    print(f\"Avg exclamation marks in ham: {df[df['label']=='ham']['exclamation_count'].mean():.2f}\")\n",
    "    print(f\"Avg caps ratio in spam: {df[df['label']=='spam']['caps_ratio'].mean():.3f}\")\n",
    "    print(f\"Avg caps ratio in ham: {df[df['label']=='ham']['caps_ratio'].mean():.3f}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_word_patterns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99474e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.preprocess import preprocess_text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying text preprocessing...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Show examples of preprocessing\n",
    "print(\"\\n📝 PREPROCESSING EXAMPLES:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. Original ({df['label'].iloc[i]}):\")\n",
    "    print(f\"   {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"   Processed:\")\n",
    "    print(f\"   {df['processed_text'].iloc[i][:100]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Check for empty processed texts\n",
    "empty_texts = df[df['processed_text'] == ''].shape[0]\n",
    "print(f\"\\n⚠️ Found {empty_texts} empty texts after preprocessing\")\n",
    "\n",
    "# Remove empty processed texts\n",
    "df = df[df['processed_text'] != ''].reset_index(drop=True)\n",
    "print(f\"✅ Final dataset size: {len(df)} emails\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and evaluation setup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Prepare features and labels\n",
    "print(\"Preparing features and labels...\")\n",
    "X = df['processed_text']\n",
    "y = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(f\"Feature vector shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Test class distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7793358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with TF-IDF\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    lowercase=True,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape (train): {X_train_vect.shape}\")\n",
    "print(f\"Feature matrix shape (test): {X_test_vect.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Show top features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "\n",
    "# Feature sparsity\n",
    "sparsity = 1 - (X_train_vect.nnz / (X_train_vect.shape[0] * X_train_vect.shape[1]))\n",
    "print(f\"Feature matrix sparsity: {sparsity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate multiple models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "model_objects = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🤖 Training {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_vect, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    y_pred_proba = model.predict_proba(X_test_vect) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    model_objects[name] = model\n",
    "    \n",
    "    print(f\"   Cross-validation: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    print(f\"   Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"   Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam'], zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 MODEL COMPARISON SUMMARY:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: {metrics['accuracy']:.4f} (CV: {metrics['cv_mean']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "models_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in models_names]\n",
    "cv_means = [results[name]['cv_mean'] for name in models_names]\n",
    "\n",
    "bars1 = axes[0,0].bar(models_names, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)\n",
    "axes[0,0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_ylim(0.8, 1.0)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, accuracy in zip(bars1, accuracies):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                   f'{accuracy:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Cross-validation comparison\n",
    "bars2 = axes[0,1].bar(models_names, cv_means, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)\n",
    "cv_stds = [results[name]['cv_std'] for name in models_names]\n",
    "axes[0,1].errorbar(models_names, cv_means, yerr=cv_stds, fmt='none', color='black', capsize=5)\n",
    "axes[0,1].set_title('Cross-Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('CV Accuracy')\n",
    "axes[0,1].set_ylim(0.8, 1.0)\n",
    "\n",
    "# Add CV values on bars\n",
    "for bar, cv_mean in zip(bars2, cv_means):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                   f'{cv_mean:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Feature importance (for Random Forest)\n",
    "if 'Random Forest' in model_objects:\n",
    "    rf_model = model_objects['Random Forest']\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    top_indices = np.argsort(feature_importance)[-15:]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importance = feature_importance[top_indices]\n",
    "    \n",
    "    axes[1,0].barh(range(len(top_features)), top_importance, color='orange', alpha=0.7)\n",
    "    axes[1,0].set_yticks(range(len(top_features)))\n",
    "    axes[1,0].set_yticklabels(top_features)\n",
    "    axes[1,0].set_title('Top 15 Features (Random Forest)', fontsize=14, fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Feature Importance')\n",
    "\n",
    "# 4. Model performance metrics\n",
    "metrics_data = []\n",
    "for name in models_names:\n",
    "    metrics_data.append([\n",
    "        results[name]['accuracy'],\n",
    "        results[name]['cv_mean'],\n",
    "        results[name]['cv_std']\n",
    "    ])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data, \n",
    "                         columns=['Test Accuracy', 'CV Mean', 'CV Std'],\n",
    "                         index=models_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(metrics_df, annot=True, fmt='.4f', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_title('Performance Metrics Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5de921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model and create detailed confusion matrix\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "best_model = model_objects[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "best_accuracy = results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"🏆 BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   CV Score: {results[best_model_name]['cv_mean']:.4f} (+/- {results[best_model_name]['cv_std']*2:.4f})\")\n",
    "\n",
    "# Create detailed confusion matrix visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'], ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = confusion_matrix(y_test, best_predictions, normalize='true')\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Greens', \n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'], ax=axes[1])\n",
    "axes[1].set_title(f'Normalized Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, best_predictions)\n",
    "recall = recall_score(y_test, best_predictions)\n",
    "f1 = f1_score(y_test, best_predictions)\n",
    "\n",
    "print(f\"\\n📊 DETAILED METRICS FOR {best_model_name}:\")\n",
    "print(f\"   Accuracy:  {best_accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# False positive and false negative analysis\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\n🔍 CONFUSION MATRIX BREAKDOWN:\")\n",
    "print(f\"   True Negatives (Ham → Ham):   {tn}\")\n",
    "print(f\"   False Positives (Ham → Spam): {fp}\")\n",
    "print(f\"   False Negatives (Spam → Ham): {fn}\")\n",
    "print(f\"   True Positives (Spam → Spam): {tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca75c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on sample emails\n",
    "sample_emails = [\n",
    "    \"Congratulations! You've won $1000! Click here to claim now!\",\n",
    "    \"Hi John, can we meet for lunch tomorrow at 1 PM?\",\n",
    "    \"FREE MONEY! URGENT! Act now and get rich quick!\",\n",
    "    \"Meeting reminder: Team standup at 10 AM in conference room.\",\n",
    "    \"WINNER! You have been selected to receive a FREE iPhone! Claim now!\",\n",
    "    \"Thanks for your email. I'll get back to you by end of week.\",\n",
    "    \"HOT SINGLES in your area! Click here for instant access!\",\n",
    "    \"Could you please review the quarterly report I sent yesterday?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 TESTING MODEL ON SAMPLE EMAILS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, email in enumerate(sample_emails, 1):\n",
    "    # Preprocess email\n",
    "    processed_email = preprocess_text(email)\n",
    "    \n",
    "    # Vectorize\n",
    "    email_vect = vectorizer.transform([processed_email])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = best_model.predict(email_vect)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        probabilities = best_model.predict_proba(email_vect)[0]\n",
    "        confidence = max(probabilities)\n",
    "        spam_prob = probabilities[1]\n",
    "    else:\n",
    "        confidence = 0.8\n",
    "        spam_prob = prediction\n",
    "    \n",
    "    result = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "    \n",
    "    print(f\"\\n{i}. Email: {email}\")\n",
    "    print(f\"   Processed: {processed_email[:80]}...\")\n",
    "    print(f\"   Prediction: {result} (Confidence: {confidence:.2%})\")\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        print(f\"   Spam Probability: {spam_prob:.3f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Feature analysis for interpretability\n",
    "if hasattr(best_model, 'coef_'):  # For Logistic Regression\n",
    "    # Get feature coefficients\n",
    "    feature_coef = best_model.coef_[0]\n",
    "    \n",
    "    # Top positive features (spam indicators)\n",
    "    top_spam_indices = np.argsort(feature_coef)[-15:]\n",
    "    top_spam_features = [(feature_names[i], feature_coef[i]) for i in top_spam_indices]\n",
    "    \n",
    "    # Top negative features (ham indicators)\n",
    "    top_ham_indices = np.argsort(feature_coef)[:15]\n",
    "    top_ham_features = [(feature_names[i], feature_coef[i]) for i in top_ham_indices]\n",
    "    \n",
    "    print(\"\\n🔍 MODEL INTERPRETATION (Logistic Regression Coefficients):\")\n",
    "    print(\"\\nTop SPAM indicators:\")\n",
    "    for feature, coef in reversed(top_spam_features):\n",
    "        print(f\"   {feature}: {coef:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop HAM indicators:\")\n",
    "    for feature, coef in top_ham_features:\n",
    "        print(f\"   {feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ NOTEBOOK ANALYSIS COMPLETE!\")\n",
    "print(f\"   Best model: {best_model_name} ({best_accuracy:.4f} accuracy)\")\n",
    "print(f\"   Total emails analyzed: {len(df)}\")\n",
    "print(f\"   Features extracted: {X_train_vect.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361a400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f5b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
